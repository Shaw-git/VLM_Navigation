{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4809c1-1f17-436b-97a9-c9c6e4470feb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61625b1-f475-4cf2-93a1-e61dcf43d654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f867d5-6c15-4554-b9d6-67633cdbe2d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Specify which BLIP-2 model you want\n",
    "# model_name = \"Salesforce/blip2-flan-t5-xl\"  # You can choose other variants like blip2-flan-t5-xl\n",
    "\n",
    "# # Download the processor and model\n",
    "# processor = Blip2Processor.from_pretrained(model_name)\n",
    "# model = Blip2ForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "# # If you have a GPU, you can move the model to it\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249faaee-5cdf-4f4c-b324-6f9e11fc02bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "default_path = os.path.expanduser(\"~/.cache/huggingface/hub/models--Salesforce--blip2-flan-t5-xl/snapshots/0eb0d3b46c14c1f8c7680bca2693baafdb90bb28/\")\n",
    "#pretrain_path = os.path.join(\"./snapshot\", f\"best_model\")\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(default_path)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(default_path, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model is loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417cbdd-e9a5-4f9a-9510-d7bb09636197",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592fd21-3f6d-403f-9ed0-08b9216cf082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, annotations_file, image_dir, processor, max_length=32, add_qa = True):\n",
    "        \"\"\"\n",
    "        Dataset for VQA fine-tuning\n",
    "        \n",
    "        Args:\n",
    "            image_dir (str): Directory containing the images\n",
    "            annotations_file (str): Path to annotations file (should contain image_id, question, answer)\n",
    "            processor (Blip2Processor): BLIP-2 processor\n",
    "            max_length (int): Maximum length for answer generation\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load annotations\n",
    "        self.samples = []\n",
    "        with open(annotations_file, 'r') as f:\n",
    "            data = json.load(f)  # Load the entire JSON file as a dictionary\n",
    "\n",
    "        for key, item in data.items():  # Iterate over key-value pairs\n",
    "            question = item['query']\n",
    "            if add_qa:\n",
    "                question = \"Question: \"+ question + \". Answer:\"\n",
    "                \n",
    "            self.samples.append({\n",
    "                'image_path': item['path'],\n",
    "                'question': question,\n",
    "                # 'answer': \"I don't want to answer Xiao Li\"\n",
    "                'answer': item['answer']\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        image_path = os.path.join(self.image_dir, f\"{item['image_path']}\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=item['question'],\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process targets\n",
    "        target = self.processor(\n",
    "            text=item['answer'],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.squeeze(0)\n",
    "        \n",
    "        labels = target.input_ids.squeeze(0)\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100  # Set padding tokens to -100 to ignore them in loss\n",
    "        \n",
    "        return {\n",
    "            \"inputs\" : inputs,\n",
    "            \"question\" : item['question'],\n",
    "            \"answer\" : item['answer'],\n",
    "            \"pixel_values\": inputs.pixel_values,\n",
    "            \"input_ids\": inputs.input_ids,\n",
    "            \"attention_mask\": inputs.attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f47e1-8760-45fd-9b74-561ff0cd8ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Let's define the LoraConfig\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"v\",'q'],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7328c-d82a-4e01-ac26-8a902b835b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, args):\n",
    "    seed_everything(args.seed)\n",
    "    \n",
    "    train_dataset = VQADataset(\n",
    "        image_dir=args.train_image_dir,\n",
    "        annotations_file=args.train_annotations,\n",
    "        processor=processor,\n",
    "        max_length=args.max_length\n",
    "    )\n",
    "    \n",
    "    val_dataset = VQADataset(\n",
    "        image_dir=args.val_image_dir,\n",
    "        annotations_file=args.val_annotations,\n",
    "        processor=processor,\n",
    "        max_length=args.max_length\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=args.num_workers\n",
    "    )\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    total_steps = len(train_dataloader) * args.num_epochs\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(args.num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{args.num_epochs} [Train]\")\n",
    "        \n",
    "        for idx, batch in zip(tqdm(range(len(train_dataloader)), desc='Training batch: ...'), train_dataloader):\n",
    "            input_ids = batch.pop('input_ids').to(device)\n",
    "            pixel_values = batch.pop('pixel_values').to(device)\n",
    "            attention_masked = batch.pop('attention_mask').to(device)\n",
    "            labels = batch.pop('labels').to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            pixel_values=pixel_values,\n",
    "                            #attention_mask = attention_masked,\n",
    "                            labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            if args.clip_grad_norm > 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_losses.append(float(loss.item()))\n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        train_loss /= len(train_dataloader)\n",
    "        #train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_pbar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{args.num_epochs} [Val]\")\n",
    "        for idx, batch in zip(tqdm(range(len(val_dataloader)), desc='Validating batch: ...'), val_dataloader):\n",
    "            input_ids = batch.pop('input_ids').to(device)\n",
    "            pixel_values = batch.pop('pixel_values').to(device)\n",
    "            attention_masked = batch.pop('attention_mask').to(device)\n",
    "            labels = batch.pop('labels').to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                        pixel_values=pixel_values,\n",
    "                        attention_mask=attention_masked,\n",
    "                        labels=labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            val_losses.append(float(loss.item()))\n",
    "            val_loss += loss.item()\n",
    "            val_pbar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "        #val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{args.num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"Saving best model with validation loss: {val_loss:.4f}\")\n",
    "            model.save_pretrained(os.path.join(args.output_dir, f\"best_model\"))\n",
    "            processor.save_pretrained(os.path.join(args.output_dir, f\"best_model\"))\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edcbf1-b896-4ff4-83cb-f707670fcc96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # General parameters\n",
    "        self.seed = 47\n",
    "        self.output_dir = \"./peft_t_aug\"\n",
    "        \n",
    "        # Data parameters\n",
    "        self.train_image_dir = \"datasets_peft\"\n",
    "        self.train_annotations = \"datasets_peft/augmented.json\"\n",
    "        self.val_image_dir = \"datasets_peft\"\n",
    "        self.val_annotations = \"datasets_peft/test_label.json\"\n",
    "        self.max_length = 32\n",
    "        \n",
    "        # Training parameters\n",
    "        self.num_epochs = 10\n",
    "        self.batch_size = 8\n",
    "        self.learning_rate = 5e-4\n",
    "        self.weight_decay = 0.01\n",
    "        self.clip_grad_norm = 1.0\n",
    "        self.num_workers = 1\n",
    "        self.save_every = 1\n",
    "        \n",
    "        # Inference parameters\n",
    "        self.model_path = \"peft_blip2flant5_vqa_finetuned/best_model\"\n",
    "        self.test_image = \"datasets_peft/bedroom/8_cuarto.jpg\"\n",
    "        self.test_question = \"Question: How to reach the bed? Answer:\"\n",
    "        self.num_beams = 5\n",
    "        self.do_sample = False\n",
    "        self.top_p = 0.9\n",
    "        self.temperature = 1.0\n",
    "        self.repetition_penalty = 1.0\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"./peft_t_aug\", exist_ok=True)\n",
    "\n",
    "# For training\n",
    "train_args = Args()\n",
    "run3_train_losses, run3_val_losses = train(peft_model, train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a52dd-1ba6-481c-b978-f7f34e0b0e41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"./peft_t_aug/Train_loss.txt\"\n",
    "# OR Mac/Linux example\n",
    "# file_path = \"/home/YourUsername/Documents/my_list.txt\"\n",
    "\n",
    "# Save to a text file with custom path\n",
    "with open(file_path, 'w') as file:\n",
    "    for item in run3_train_losses:\n",
    "        file.write(f\"{item}\\n\")\n",
    "\n",
    "print(f\"List saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9190f-aa12-44de-b0da-ace8ff562d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"./peft_t_aug/Val_loss.txt\"\n",
    "# OR Mac/Linux example\n",
    "# file_path = \"/home/YourUsername/Documents/my_list.txt\"\n",
    "\n",
    "# Save to a text file with custom path\n",
    "with open(file_path, 'w') as file:\n",
    "    for item in run3_val_losses:\n",
    "        file.write(f\"{item}\\n\")\n",
    "\n",
    "print(f\"List saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761281b4-c719-44ff-92d5-f5f667723015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "args = Args()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, 2001), run3_train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(range(1, 201), run3_val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "loss_plot_path = os.path.join(args.output_dir, \"training_loss_plot.png\")\n",
    "plt.savefig(loss_plot_path)\n",
    "print(f\"Loss plot saved at {loss_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f351ef8-a455-4031-8224-1fad1aa712b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bert_score import score\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy for linguistic analysis\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def build_directional_synonym_dictionary():\n",
    "    \"\"\"\n",
    "    Build an enhanced comprehensive dictionary of directional terms with synonym groups\n",
    "    based on analysis of navigation instruction datasets\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of directional terms organized by semantic groups\n",
    "    \"\"\"\n",
    "    # Create semantic groups of functionally equivalent directional terms\n",
    "    directional_dict = {\n",
    "        # Basic directional terms with synonym groups\n",
    "        'basic_directions': {\n",
    "            'left_group': ['left', 'leftward', 'leftside', 'to the left'],\n",
    "            'right_group': ['right', 'rightward', 'rightside', 'to the right'],\n",
    "            'front_group': ['front', 'ahead', 'forward', 'straight ahead', 'directly ahead',\n",
    "                           'in front', 'straight forward', 'onward'],\n",
    "            'back_group': ['back', 'behind', 'backward', 'rear', 'backwards'],\n",
    "            'up_group': ['up', 'upward', 'upper', 'above', 'upstairs'],\n",
    "            'down_group': ['down', 'downward', 'lower', 'below', 'downstairs'],\n",
    "            'center_group': ['center', 'middle', 'central'],\n",
    "        },\n",
    "\n",
    "        # Movement verbs with expanded synonym groups\n",
    "        'movement_verbs': {\n",
    "            'walk_group': ['walk', 'move', 'go', 'proceed', 'head', 'continue', 'step',\n",
    "                          'advance', 'progress', 'press on', 'forge ahead'],\n",
    "            'turn_group': ['turn', 'rotate', 'pivot', 'veer', 'swing', 'swerve', 'wheel'],\n",
    "            'follow_group': ['follow', 'take', 'use', 'pursue'],\n",
    "            'find_group': ['find', 'locate', 'look for', 'seek', 'spot', 'identify'],\n",
    "            'enter_exit_group': ['enter', 'exit', 'access', 'leave', 'go in', 'go out'],\n",
    "            'pass_group': ['pass', 'bypass', 'go past', 'move past', 'overtake'],\n",
    "            'sit_group': ['sit', 'take a seat', 'be seated'],\n",
    "        },\n",
    "\n",
    "        # Relative positions with synonym groups\n",
    "        'relative_positions': {\n",
    "            'left_side_group': ['on your left', 'to your left', 'on the left', 'to the left side'],\n",
    "            'right_side_group': ['on your right', 'to your right', 'on the right', 'to the right side'],\n",
    "            'front_side_group': ['in front of you', 'directly in front', 'straight ahead of you',\n",
    "                               'directly ahead', 'straight in front'],\n",
    "            'back_side_group': ['behind you', 'to your rear', 'at your back'],\n",
    "            'near_group': ['near', 'close to', 'nearby', 'adjacent to', 'next to', 'beside'],\n",
    "            'beyond_group': ['beyond', 'past', 'further than', 'after'],\n",
    "            'between_group': ['between', 'amid', 'among', 'in between'],\n",
    "            'along_group': ['along', 'alongside', 'by', 'across'],\n",
    "        },\n",
    "\n",
    "        # Compound directions with expanded synonym groups\n",
    "        'compound_directions': {\n",
    "            'turn_left_group': ['turn left', 'make a left', 'make a left turn', 'veer left',\n",
    "                               'take a left', 'take a left turn', 'hang a left', 'bear left'],\n",
    "            'turn_right_group': ['turn right', 'make a right', 'make a right turn', 'veer right',\n",
    "                                'take a right', 'take a right turn', 'hang a right', 'bear right'],\n",
    "            'go_straight_group': ['go straight', 'continue straight', 'proceed straight',\n",
    "                                 'head straight', 'walk straight', 'move straight ahead',\n",
    "                                 'move forward', 'advance forward'],\n",
    "            'turn_around_group': ['turn around', 'turn back', 'rotate 180 degrees', 'make a u-turn'],\n",
    "            'walk_toward_group': ['walk toward', 'move toward', 'proceed toward', 'head toward',\n",
    "                                 'go toward', 'approach', 'walk to', 'move to'],\n",
    "            'move_right_group': ['move right', 'go right', 'head right', 'proceed right'],\n",
    "            'move_left_group': ['move left', 'go left', 'head left', 'proceed left'],\n",
    "        },\n",
    "\n",
    "        # Landmark references often used with directions\n",
    "        'landmark_references': {\n",
    "            'counter_group': ['counter', 'bar counter', 'bar', 'desk'],\n",
    "            'door_group': ['door', 'doorway', 'entrance', 'exit', 'gateway'],\n",
    "            'seating_group': ['seat', 'chair', 'stool', 'bench', 'seating area', 'seating section'],\n",
    "            'wall_group': ['wall', 'side wall', 'back wall', 'front wall'],\n",
    "            'room_group': ['room', 'area', 'section', 'space', 'hall'],\n",
    "            'table_group': ['table', 'desk', 'workspace'],\n",
    "            'stage_group': ['stage', 'platform', 'podium'],\n",
    "            'aisle_group': ['aisle', 'corridor', 'pathway', 'walkway'],\n",
    "        },\n",
    "\n",
    "        # Navigation action synonyms\n",
    "        'navigation_actions': {\n",
    "            'proceed_group': ['proceed', 'continue', 'advance', 'progress', 'move on',\n",
    "                             'go ahead', 'move forward', 'press on', 'forge ahead'],\n",
    "            'navigate_group': ['navigate', 'guide', 'steer', 'pilot', 'direct', 'maneuver'],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Flatten the dictionary for easier term detection\n",
    "    flat_dict = {}\n",
    "    for category, group_dict in directional_dict.items():\n",
    "        for group_name, terms in group_dict.items():\n",
    "            flat_dict[group_name] = terms\n",
    "\n",
    "    return directional_dict, flat_dict\n",
    "\n",
    "def detect_directional_terms_with_spacy(text, flat_dict, nlp):\n",
    "    \"\"\"\n",
    "    Enhanced detection of directional terms with better synonym handling\n",
    "    \"\"\"\n",
    "    doc = nlp(text.lower())\n",
    "    found_terms = []\n",
    "\n",
    "    # First check for multi-word terms\n",
    "    text_lower = text.lower()\n",
    "    for group_name, terms in flat_dict.items():\n",
    "        for term in terms:\n",
    "            if ' ' in term and term in text_lower:\n",
    "                found_terms.append((term, group_name))\n",
    "                # Mark the text to avoid double-counting\n",
    "                text_lower = text_lower.replace(term, ' '*len(term))\n",
    "\n",
    "\n",
    "    for token in doc:\n",
    "        term = token.text\n",
    "        for group_name, terms in flat_dict.items():\n",
    "            if term in terms:\n",
    "                found_terms.append((term, group_name))\n",
    "                break\n",
    "\n",
    "          # Adjust similarity threshold for movement verbs and compound directions\n",
    "        if group_name in ['move_right_group', 'move_left_group', 'go_straight_group']:\n",
    "            threshold = 0.75  # Lower threshold for these specific groups\n",
    "        elif group_name.startswith('turn_') or group_name.endswith('_group'):\n",
    "            threshold = 0.85\n",
    "        else:\n",
    "            threshold = 0.8\n",
    "\n",
    "        for dict_term in terms:\n",
    "            if ' ' not in dict_term and nlp(term).has_vector and nlp(dict_term).has_vector:\n",
    "                similarity = nlp(term).similarity(nlp(dict_term))\n",
    "                if similarity > threshold:\n",
    "                    found_terms.append((term, group_name))\n",
    "                    break\n",
    "\n",
    "    return found_terms\n",
    "\n",
    "def has_directional_conflict(ref_text, cand_text):\n",
    "    \"\"\"\n",
    "    Check if there are conflicting directional instructions\n",
    "\n",
    "    Args:\n",
    "        ref_text: Reference (ground truth) text\n",
    "        cand_text: Candidate (prediction) text\n",
    "\n",
    "    Returns:\n",
    "        Boolean indicating if conflicting directions exist\n",
    "    \"\"\"\n",
    "    opposing_pairs = [\n",
    "        ('left', 'right'),\n",
    "        ('front', 'back'),\n",
    "        ('ahead', 'behind'),\n",
    "        ('forward', 'backward'),\n",
    "        ('up', 'down')\n",
    "    ]\n",
    "\n",
    "    ref_text, cand_text = ref_text.lower(), cand_text.lower()\n",
    "\n",
    "    for term1, term2 in opposing_pairs:\n",
    "        ref_has_term1 = re.search(r'\\b' + re.escape(term1) + r'\\b', ref_text) is not None\n",
    "        ref_has_term2 = re.search(r'\\b' + re.escape(term2) + r'\\b', ref_text) is not None\n",
    "        cand_has_term1 = re.search(r'\\b' + re.escape(term1) + r'\\b', cand_text) is not None\n",
    "        cand_has_term2 = re.search(r'\\b' + re.escape(term2) + r'\\b', cand_text) is not None\n",
    "\n",
    "        # Check for contradictory directions\n",
    "        if (ref_has_term1 and not ref_has_term2) and (cand_has_term2 and not cand_has_term1):\n",
    "            return True\n",
    "        if (ref_has_term2 and not ref_has_term1) and (cand_has_term1 and not cand_has_term2):\n",
    "            return True\n",
    "\n",
    "    # Check compound directional conflicts\n",
    "    compound_conflicts = [\n",
    "        ('turn left', 'turn right'),\n",
    "        ('on your left', 'on your right'),\n",
    "        ('to your left', 'to your right')\n",
    "    ]\n",
    "\n",
    "    for comp1, comp2 in compound_conflicts:\n",
    "        ref_has_comp1 = comp1 in ref_text\n",
    "        ref_has_comp2 = comp2 in ref_text\n",
    "        cand_has_comp1 = comp1 in cand_text\n",
    "        cand_has_comp2 = comp2 in cand_text\n",
    "\n",
    "        if (ref_has_comp1 and not ref_has_comp2) and (cand_has_comp2 and not cand_has_comp1):\n",
    "            return True\n",
    "        if (ref_has_comp2 and not ref_has_comp1) and (cand_has_comp1 and not cand_has_comp2):\n",
    "            return True\n",
    "\n",
    "    # Special case for turn around (180°) vs. rotate 360°\n",
    "    if ('turn around' in ref_text and '360' in cand_text):\n",
    "        return True\n",
    "\n",
    "    # Front vs back position conflicts\n",
    "    if ('at the back' in ref_text and 'at the front' in cand_text) or \\\n",
    "       ('at the front' in ref_text and 'at the back' in cand_text):\n",
    "        return True\n",
    "\n",
    "    # Behind vs in front conflicts\n",
    "    if ('behind you' in ref_text and 'in front of' in cand_text) or \\\n",
    "       ('in front of' in ref_text and 'behind you' in cand_text):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def extract_directional_sequence(doc):\n",
    "    directional_verbs = {'walk', 'move', 'go', 'turn', 'proceed', 'head', 'continue', 'enter'}\n",
    "    sequence = []\n",
    "\n",
    "    # Extract verbs in document order\n",
    "    for token in doc:\n",
    "        if token.lemma_ in directional_verbs:\n",
    "            direction = ' '.join([child.text for child in token.children\n",
    "                                if child.dep_ in ('prep', 'dobj')])\n",
    "            sequence.append((token.lemma_, direction))\n",
    "\n",
    "    return sequence\n",
    "\n",
    "    # ADD: Sort by original token order instead of dependency order\n",
    "    sequence.sort(key=lambda x: x[2])  # Sort by token position\n",
    "\n",
    "    # REMOVE OLD RETURN AND REPLACE WITH:\n",
    "    return [(item[0], item[1]) for item in sequence]  # Return ordered actions\n",
    "\n",
    "\n",
    "\n",
    "def calculate_directional_flow_similarity(ref_text, cand_text, nlp):\n",
    "    \"\"\"\n",
    "    Calculate similarity in directional flow/sequence between two texts using spaCy\n",
    "\n",
    "    Args:\n",
    "        ref_text: Reference text\n",
    "        cand_text: Candidate text\n",
    "        nlp: spaCy NLP model\n",
    "\n",
    "    Returns:\n",
    "        Float from 0.0 to 1.0 indicating flow similarity\n",
    "    \"\"\"\n",
    "\n",
    "    # Add at the beginning\n",
    "    if has_sequence_conflict(ref_text, cand_text, nlp):\n",
    "        return 0.0  # Zero score for sequence conflicts\n",
    "\n",
    "    # Parse texts with spaCy\n",
    "    ref_doc = nlp(ref_text.lower())\n",
    "    cand_doc = nlp(cand_text.lower())\n",
    "\n",
    "    # Extract directional sequences from the texts\n",
    "    ref_sequence = extract_directional_sequence(ref_doc)\n",
    "    cand_sequence = extract_directional_sequence(cand_doc)\n",
    "\n",
    "\n",
    "    # ADD STRICT ORDER VALIDATION AT BEGINNING\n",
    "    if len(ref_sequence) >= 2 and len(cand_sequence) >= 2:\n",
    "        ref_first_actions = [item[0] for item in ref_sequence[:2]]\n",
    "        cand_first_actions = [item[0] for item in cand_sequence[:2]]\n",
    "\n",
    "        # Apply penalty if first two actions differ in order\n",
    "        if ref_first_actions != cand_first_actions:\n",
    "            return 0.2  # Hard penalty (original range: 0-1)\n",
    "\n",
    "\n",
    "\n",
    " # ADD THIS: Check for sequence order mismatch\n",
    "    sequence_order_match = True\n",
    "    if len(ref_sequence) >= 2 and len(cand_sequence) >= 2:\n",
    "        # Check if primary actions are reversed\n",
    "        actions_reversed = False\n",
    "        ref_verbs = [item[0] for item in ref_sequence[:2]]\n",
    "        cand_verbs = [item[0] for item in cand_sequence[:2]]\n",
    "\n",
    "        # Check if first two actions are swapped\n",
    "        if ref_verbs[0] == cand_verbs[1] and ref_verbs[1] == cand_verbs[0]:\n",
    "            actions_reversed = True\n",
    "            sequence_order_match = False\n",
    "\n",
    "        # Check if directional objects are swapped\n",
    "        ref_dirs = [item[1] for item in ref_sequence[:2]]\n",
    "        cand_dirs = [item[1] for item in cand_sequence[:2]]\n",
    "        if ref_dirs[0] in cand_dirs[1] or ref_dirs[1] in cand_dirs[0]:\n",
    "            sequence_order_match = False\n",
    "\n",
    "    # Apply severe penalty for sequence errors\n",
    "    if not sequence_order_match:\n",
    "        return 0.1  # Return very low score for sequence order mismatch\n",
    "\n",
    "    # Continue with existing similarity calculation...\n",
    "    # [rest of the original function]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # If no sequences found, return neutral score\n",
    "    if not ref_sequence or not cand_sequence:\n",
    "        return 0.5\n",
    "\n",
    "    # Calculate sequence similarity\n",
    "    max_len = max(len(ref_sequence), len(cand_sequence))\n",
    "    if max_len == 0:\n",
    "        return 0.5\n",
    "\n",
    "    # Count matching steps in sequence with word vector similarity\n",
    "    similarity_sum = 0\n",
    "    for i in range(min(len(ref_sequence), len(cand_sequence))):\n",
    "        ref_verb, ref_dir = ref_sequence[i]\n",
    "        cand_verb, cand_dir = cand_sequence[i]\n",
    "\n",
    "        # Calculate verb similarity\n",
    "        verb_sim = nlp(ref_verb).similarity(nlp(cand_verb)) if nlp(ref_verb).has_vector and nlp(cand_verb).has_vector else 0\n",
    "\n",
    "        # Calculate direction similarity\n",
    "        dir_sim = nlp(ref_dir).similarity(nlp(cand_dir)) if nlp(ref_dir).has_vector and nlp(cand_dir).has_vector else 0\n",
    "\n",
    "        # Combined similarity for this step\n",
    "        step_sim = (verb_sim + dir_sim) / 2\n",
    "        similarity_sum += step_sim\n",
    "\n",
    "    # Normalize by sequence length\n",
    "    return similarity_sum / min(len(ref_sequence), len(cand_sequence))\n",
    "\n",
    "def has_sequence_conflict(ref_text, cand_text, nlp):\n",
    "    \"\"\"\n",
    "    Check if there are conflicting sequence orders in the instructions\n",
    "\n",
    "    Args:\n",
    "        ref_text: Reference (ground truth) text\n",
    "        cand_text: Candidate (prediction) text\n",
    "        nlp: spaCy NLP model\n",
    "\n",
    "    Returns:\n",
    "        Boolean indicating if sequence order conflicts exist\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # ADD THESE LINES FOR TEMPORAL MARKER CHECK\n",
    "    temporal_markers = {'then', 'after', 'next', 'first', 'second'}\n",
    "\n",
    "    cand_has_markers = any(marker in cand_text.lower() for marker in temporal_markers)\n",
    "    ref_has_markers = any(marker in ref_text.lower() for marker in temporal_markers)\n",
    "\n",
    "    # Check for temporal markers in either text\n",
    "    has_temporal = any(marker in ref_text.lower() or marker in cand_text.lower()\n",
    "                      for marker in temporal_markers)\n",
    "\n",
    "\n",
    "    # Parse texts\n",
    "    ref_doc = nlp(ref_text.lower())\n",
    "    cand_doc = nlp(cand_text.lower())\n",
    "\n",
    "    # MODIFY ACTION REVERSAL CHECK\n",
    "    ref_sequence = extract_directional_sequence(nlp(ref_text.lower()))\n",
    "    cand_sequence = extract_directional_sequence(nlp(cand_text.lower()))\n",
    "\n",
    "    # ADD STRICT ACTION ORDER CHECK\n",
    "    if len(ref_sequence) >= 2 and len(cand_sequence) >= 2:\n",
    "        ref_verbs = [item[0] for item in ref_sequence[:2]]\n",
    "        cand_verbs = [item[0] for item in cand_sequence[:2]]\n",
    "\n",
    "    if len(ref_sequence) < 2 or len(cand_sequence) < 2:\n",
    "        return False\n",
    "\n",
    "    # Check if first two actions are swapped with temporal markers\n",
    "    if has_temporal:\n",
    "        ref_verbs = [item[0] for item in ref_sequence[:2]]\n",
    "        cand_verbs = [item[0] for item in cand_sequence[:2]]\n",
    "\n",
    "        if ref_verbs == list(reversed(cand_verbs)):\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "    # Check verb order mismatch with temporal markers\n",
    "    if ref_verbs != cand_verbs and (cand_has_markers or ref_has_markers):\n",
    "        return True\n",
    "\n",
    "\n",
    "    # Check if actions appear in reverse order\n",
    "    reversed_actions = False\n",
    "    for i in range(min(len(ref_sequence)-1, len(cand_sequence)-1)):\n",
    "        for j in range(i+1, min(len(ref_sequence), len(cand_sequence))):\n",
    "            if ref_sequence[i][0] == cand_sequence[j][0] and ref_sequence[j][0] == cand_sequence[i][0]:\n",
    "                reversed_actions = True\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    # ENHANCE LANDMARK ORDER CHECK\n",
    "    ref_landmarks = [item[1] for item in ref_sequence if item[1]]\n",
    "    cand_landmarks = [item[1] for item in cand_sequence if item[1]]\n",
    "\n",
    "    if len(ref_landmarks) >= 2 and len(cand_landmarks) >= 2:\n",
    "      # Check if first two landmarks are swapped\n",
    "      if (ref_landmarks[0] in cand_landmarks[1] and\n",
    "          ref_landmarks[1] in cand_landmarks[0]):\n",
    "          return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "    # Check for landmark action reversals (e.g., \"enter door then hallway\" vs \"hallway then door\")\n",
    "    landmark_reversal = False\n",
    "\n",
    "\n",
    "    for landmark in ['door', 'hallway', 'corridor', 'room', 'stairs', 'elevator']:\n",
    "        ref_indices = [i for i, dir in enumerate(ref_landmarks) if landmark in dir.lower()]\n",
    "        cand_indices = [i for i, dir in enumerate(cand_landmarks) if landmark in dir.lower()]\n",
    "\n",
    "        if ref_indices and cand_indices:\n",
    "            # Check if landmark appears in different order\n",
    "            if ref_indices[0] < len(ref_sequence)//2 and cand_indices[0] >= len(cand_sequence)//2:\n",
    "                landmark_reversal = True\n",
    "                break\n",
    "\n",
    "    return reversed_actions or landmark_reversal\n",
    "\n",
    "def calculate_enhanced_directional_score(ref_text, cand_text, directional_dict, flat_dict, nlp):\n",
    "    \"\"\"\n",
    "    Calculate improved similarity score focusing on semantic directional equivalence\n",
    "    using spaCy for semantic similarity\n",
    "    \"\"\"\n",
    "    # Check for directional conflict or sequence conflict first\n",
    "    if (has_directional_conflict(ref_text, cand_text) or\n",
    "            has_sequence_conflict(ref_text, cand_text, nlp)):\n",
    "            return 0.0, [], [], []  # Zero score for sequence/directional conflicts\n",
    "\n",
    "    # Detect terms and their semantic groups using spaCy\n",
    "    ref_terms_with_groups = detect_directional_terms_with_spacy(ref_text, flat_dict, nlp)\n",
    "    cand_terms_with_groups = detect_directional_terms_with_spacy(cand_text, flat_dict, nlp)\n",
    "\n",
    "    # If no directional terms in reference, return 1.0\n",
    "    if not ref_terms_with_groups:\n",
    "        return 1.0, [], [], []\n",
    "\n",
    "    # Extract just the terms for output purposes\n",
    "    ref_terms = [term for term, _ in ref_terms_with_groups]\n",
    "    cand_terms = [term for term, _ in cand_terms_with_groups]\n",
    "\n",
    "    # Extract the groups\n",
    "    ref_groups = [group for _, group in ref_terms_with_groups]\n",
    "    cand_groups = [group for _, group in cand_terms_with_groups]\n",
    "\n",
    "    # Count unique semantic groups in each text\n",
    "    ref_group_counts = Counter(ref_groups)\n",
    "    cand_group_counts = Counter(cand_groups)\n",
    "\n",
    "    # Find all unique groups\n",
    "    all_groups = set(ref_group_counts.keys()) | set(cand_group_counts.keys())\n",
    "\n",
    "    # Check for special cases of functionally equivalent instructions\n",
    "    special_case_boost = 0.0\n",
    "\n",
    "    # Case 1: \"Move right\" vs \"Go right\" type equivalence\n",
    "    move_go_equivalence = False\n",
    "    if ('move_right_group' in ref_groups and 'move_right_group' in cand_groups) or \\\n",
    "       ('move_left_group' in ref_groups and 'move_left_group' in cand_groups):\n",
    "        move_go_equivalence = True\n",
    "        special_case_boost = 0.3  # Significant boost\n",
    "\n",
    "    # Case 2: \"Turn left\" vs \"Take a left turn\" type equivalence\n",
    "    turn_take_equivalence = False\n",
    "    if ('turn_left_group' in ref_groups and 'turn_left_group' in cand_groups) or \\\n",
    "       ('turn_right_group' in ref_groups and 'turn_right_group' in cand_groups):\n",
    "        turn_take_equivalence = True\n",
    "        special_case_boost = 0.3  # Significant boost\n",
    "\n",
    "    # Assign weights to different categories\n",
    "    category_weights = {\n",
    "        'basic_directions': 1.0,\n",
    "        'compound_directions': 1.5,  # Increased weight\n",
    "        'relative_positions': 0.9,\n",
    "        'movement_verbs': 1.2,  # Increased weight\n",
    "        'landmark_references': 0.7,\n",
    "        'navigation_actions': 1.1  # Added category\n",
    "    }\n",
    "\n",
    "    # Calculate weighted semantic matching score\n",
    "    total_weight = 0\n",
    "    matched_weight = 0\n",
    "    matched_groups = []\n",
    "\n",
    "    # Determine which group belongs to which category\n",
    "    group_to_category = {}\n",
    "    for category, group_dict in directional_dict.items():\n",
    "        for group_name in group_dict:\n",
    "            group_to_category[group_name] = category\n",
    "\n",
    "    # Calculate scores based on matched semantic groups\n",
    "    for group in all_groups:\n",
    "        # Get category for this group\n",
    "        category = group_to_category.get(group, 'basic_directions')  # Default to basic if not found\n",
    "        weight = category_weights.get(category, 1.0)\n",
    "\n",
    "        # Extra weight for move/go and turn/take equivalence groups\n",
    "        if (move_go_equivalence and group in ['move_right_group', 'move_left_group']) or \\\n",
    "           (turn_take_equivalence and group in ['turn_left_group', 'turn_right_group']):\n",
    "            weight *= 1.5  # 50% more weight for these specific groups\n",
    "\n",
    "        # Count occurrences in each text\n",
    "        ref_count = ref_group_counts.get(group, 0)\n",
    "        cand_count = cand_group_counts.get(group, 0)\n",
    "\n",
    "        # Add to total weight\n",
    "        total_weight += weight * max(ref_count, cand_count)\n",
    "\n",
    "        # Add to matched weight (using the minimum count as match)\n",
    "        if min(ref_count, cand_count) > 0:\n",
    "            matched_weight += weight * min(ref_count, cand_count)\n",
    "            matched_groups.append(group)\n",
    "\n",
    "    # Calculate final score\n",
    "    if total_weight > 0:\n",
    "        similarity = matched_weight / total_weight\n",
    "    else:\n",
    "        similarity = 0.0\n",
    "\n",
    "    # Apply directional flow bonus using spaCy\n",
    "    flow_bonus = calculate_directional_flow_similarity(ref_text, cand_text, nlp)\n",
    "\n",
    "    # Calculate overall semantic similarity using spaCy\n",
    "    ref_doc = nlp(ref_text)\n",
    "    cand_doc = nlp(cand_text)\n",
    "    semantic_similarity = ref_doc.similarity(cand_doc)\n",
    "\n",
    "    # Apply special case boost for functionally equivalent instructions\n",
    "    # Combine all scores into final directional score with special case boost\n",
    "    final_score = 0.4 * similarity + 0.3 * flow_bonus + 0.1 * semantic_similarity + special_case_boost\n",
    "\n",
    "    # For very short, functionally equivalent instructions, provide an additional boost\n",
    "    if len(ref_text.split()) <= 3 and len(cand_text.split()) <= 4 and special_case_boost > 0:\n",
    "        final_score += 0.1  # Additional boost for short, equivalent instructions\n",
    "\n",
    "    # Ensure score is between 0 and 1\n",
    "    final_score = max(0.0, min(1.0, final_score))\n",
    "\n",
    "    return final_score, ref_terms, cand_terms, matched_groups\n",
    "\n",
    "\n",
    "def enhanced_directional_weighted_bertscore(refs, cands, directional_weight, model_name=\"microsoft/deberta-xlarge-mnli\"):\n",
    "    \"\"\"\n",
    "    Calculate BERTScore with enhanced directional term weighting using spaCy\n",
    "\n",
    "    Args:\n",
    "        refs: List of reference (ground truth) texts\n",
    "        cands: List of candidate (prediction) texts\n",
    "        directional_weight: Weight to give directional terms (0.0-1.0)\n",
    "        model_name: BERT model to use\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with results\n",
    "    \"\"\"\n",
    "    print(f\"Computing Enhanced Directional-Weighted BERTScore with {model_name} and spaCy...\")\n",
    "\n",
    "    # Build directional dictionary with synonym groups\n",
    "    directional_dict, flat_dict = build_directional_synonym_dictionary()\n",
    "\n",
    "    # Print directional term dictionary stats\n",
    "    print(\"\\nEnhanced Directional Dictionary:\")\n",
    "    total_terms = 0\n",
    "    for category, group_dict in directional_dict.items():\n",
    "        category_terms = sum(len(terms) for terms in group_dict.values())\n",
    "        total_terms += category_terms\n",
    "        print(f\"  {category}: {len(group_dict)} groups, {category_terms} terms\")\n",
    "        # Print example groups\n",
    "        example_groups = list(group_dict.items())[:2]\n",
    "        for group_name, terms in example_groups:\n",
    "            print(f\"    {group_name}: {', '.join(terms[:3])}{'...' if len(terms) > 3 else ''}\")\n",
    "\n",
    "    print(f\"  Total: {total_terms} directional terms in {sum(len(group_dict) for group_dict in directional_dict.values())} semantic groups\")\n",
    "\n",
    "    # Calculate standard BERTScore\n",
    "    P, R, F1 = score(cands, refs, lang=\"en\", model_type=model_name, verbose=True)\n",
    "\n",
    "    # Convert to numpy\n",
    "    P_np = P.numpy()\n",
    "    R_np = R.numpy()\n",
    "    F1_np = F1.numpy()\n",
    "\n",
    "    # Calculate enhanced directional scores with spaCy\n",
    "    directional_scores = []\n",
    "    weighted_scores = []\n",
    "    conflicts = []\n",
    "    ref_dir_terms = []\n",
    "    cand_dir_terms = []\n",
    "    matched_dir_groups = []\n",
    "\n",
    "    sequence_conflicts = []\n",
    "    for i, (ref, cand) in enumerate(zip(refs, cands)):\n",
    "        # Calculate directional score using spaCy\n",
    "        dir_score, ref_terms, cand_terms, matched_groups = calculate_enhanced_directional_score(\n",
    "            ref, cand, directional_dict, flat_dict, nlp\n",
    "        )\n",
    "        directional_scores.append(dir_score)\n",
    "        ref_dir_terms.append(ref_terms)\n",
    "        cand_dir_terms.append(cand_terms)\n",
    "        matched_dir_groups.append(matched_groups)\n",
    "\n",
    "        # Check for conflicts\n",
    "        conflict = has_directional_conflict(ref, cand)\n",
    "        conflicts.append(conflict)\n",
    "\n",
    "        # Add sequence conflict check\n",
    "        seq_conflict = has_sequence_conflict(ref, cand, nlp)\n",
    "        sequence_conflicts.append(seq_conflict)\n",
    "\n",
    "        # Calculate weighted score\n",
    "        weighted_f1 = (1 - directional_weight) * F1_np[i] + directional_weight * dir_score\n",
    "        weighted_scores.append(weighted_f1)\n",
    "\n",
    "    # Create a dataframe with results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Pair_ID': range(1, len(refs) + 1),\n",
    "        'Ground Truth': refs,\n",
    "        'Prediction': cands,\n",
    "        'BERT_Precision': P_np,\n",
    "        'BERT_Recall': R_np,\n",
    "        'BERT_F1': F1_np,\n",
    "        'Enhanced_Directional_Score': directional_scores,\n",
    "        'Directional_Conflict': conflicts,\n",
    "        'Weighted_F1': weighted_scores,\n",
    "        'Ref_Directional_Terms': [', '.join(terms) for terms in ref_dir_terms],\n",
    "        'Cand_Directional_Terms': [', '.join(terms) for terms in cand_dir_terms],\n",
    "        'Matched_Semantic_Groups': [', '.join(groups) for groups in matched_dir_groups],\n",
    "        'Sequence_Conflict': sequence_conflicts  # Add this line\n",
    "    })\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_bert_f1 = np.mean(F1_np)\n",
    "    avg_dir_score = np.mean(directional_scores)\n",
    "    avg_weighted_f1 = np.mean(weighted_scores)\n",
    "    conflict_count = sum(conflicts)\n",
    "\n",
    "    print(f\"\\nAverage BERT F1 Score: {avg_bert_f1:.4f}\")\n",
    "    print(f\"Average Enhanced Directional Score: {avg_dir_score:.4f}\")\n",
    "    print(f\"Average Weighted F1 Score: {avg_weighted_f1:.4f}\")\n",
    "    print(f\"Number of directional conflicts detected: {conflict_count}\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def visualize_results(results_df, directional_weight):\n",
    "    \"\"\"\n",
    "    Visualize the evaluation results\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame with results\n",
    "        directional_weight: Weight given to directional terms\n",
    "\n",
    "    Returns:\n",
    "        None (displays visualization)\n",
    "    \"\"\"\n",
    "    # Extract data for visualization\n",
    "    conflicts = results_df['Directional_Conflict']\n",
    "    conflict_count = sum(conflicts)\n",
    "\n",
    "    # Visualize the results\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Create bar chart for scores with color coding for conflicts\n",
    "    bar_colors = ['red' if conflict else 'skyblue' for conflict in conflicts]\n",
    "    x = np.arange(len(results_df))\n",
    "    width = 0.3\n",
    "\n",
    "    plt.bar(x - width, results_df['BERT_F1'], width=width, label='BERT F1', color='skyblue', alpha=0.7)\n",
    "    plt.bar(x, results_df['Enhanced_Directional_Score'], width=width, label='Enhanced Directional Score', color='green', alpha=0.7)\n",
    "    plt.bar(x + width, results_df['Weighted_F1'], width=width, label='Weighted F1', color=bar_colors, alpha=0.7)\n",
    "\n",
    "    # Add average score lines\n",
    "    avg_bert_f1 = results_df['BERT_F1'].mean()\n",
    "    avg_dir_score = results_df['Enhanced_Directional_Score'].mean()\n",
    "    avg_weighted_f1 = results_df['Weighted_F1'].mean()\n",
    "\n",
    "    plt.axhline(y=avg_bert_f1, color='skyblue', linestyle='--', label=f'Avg BERT F1: {avg_bert_f1:.4f}')\n",
    "    plt.axhline(y=avg_dir_score, color='green', linestyle='--', label=f'Avg Directional: {avg_dir_score:.4f}')\n",
    "    plt.axhline(y=avg_weighted_f1, color='red', linestyle='--', label=f'Avg Weighted F1: {avg_weighted_f1:.4f}')\n",
    "\n",
    "    plt.xlabel('Pair ID')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'Enhanced BERTScore with Directional Weighting (Weight={directional_weight})')\n",
    "    plt.xticks(x, results_df['Pair_ID'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add note for conflicting directions and sequences\n",
    "    if conflict_count > 0:\n",
    "        conflict_ids = [i+1 for i, c in enumerate(conflicts) if c]\n",
    "        seq_conflict_ids = [i+1 for i, c in enumerate(results_df['Sequence_Conflict']) if c]\n",
    "\n",
    "\n",
    "        conflict_text = []\n",
    "        if conflict_count > 0:\n",
    "            conflict_text.append(f\"Directional conflicts: {conflict_ids}\")\n",
    "\n",
    "\n",
    "\n",
    "    # ADD SEQUENCE CONFLICT TRACKING\n",
    "        seq_conflicts = results_df[results_df['Sequence_Conflict']].index.tolist()\n",
    "        if seq_conflicts:\n",
    "            conflict_text.append(f\"Sequence conflicts: {[x+1 for x in seq_conflicts]}\")\n",
    "\n",
    "        if conflict_text:\n",
    "            plt.figtext(0.5, 0.01, \"\\n\".join(conflict_text),\n",
    "                      ha=\"center\", fontsize=10, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":5})\n",
    "\n",
    "\n",
    "\n",
    "        conflict_text = f\"Red bars indicate directional conflicts in pairs: {conflict_ids}\"\n",
    "        if seq_conflict_ids:\n",
    "            conflict_text += f\"\\nOrange bars indicate sequence order conflicts in pairs: {seq_conflict_ids}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_directional_differences(results_df, n=3):\n",
    "    \"\"\"\n",
    "    Analyze pairs with the largest differences between BERT F1 and directional scores\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame with BERTScore results\n",
    "        n: Number of examples to show\n",
    "\n",
    "    Returns:\n",
    "        None (prints analysis)\n",
    "    \"\"\"\n",
    "    # Calculate difference between scores\n",
    "    results_df['Score_Difference'] = abs(results_df['BERT_F1'] - results_df['Enhanced_Directional_Score'])\n",
    "\n",
    "    # Sort by difference\n",
    "    largest_diff = results_df.sort_values(by='Score_Difference', ascending=False).head(n)\n",
    "\n",
    "    print(f\"\\nTop {n} Pairs with Largest Differences Between BERT F1 and Directional Scores:\")\n",
    "    for i, row in largest_diff.iterrows():\n",
    "        print(f\"Pair {row['Pair_ID']} (Difference: {row['Score_Difference']:.4f}):\")\n",
    "        print(f\"  Ground Truth: {row['Ground Truth']}\")\n",
    "        print(f\"  Prediction: {row['Prediction']}\")\n",
    "        print(f\"  BERT F1: {row['BERT_F1']:.4f}, Enhanced Directional Score: {row['Enhanced_Directional_Score']:.4f}\")\n",
    "        print(f\"  Ground Truth Directional Terms: {row['Ref_Directional_Terms']}\")\n",
    "        print(f\"  Prediction Directional Terms: {row['Cand_Directional_Terms']}\")\n",
    "        print(f\"  Matched Semantic Groups: {row['Matched_Semantic_Groups']}\")\n",
    "        print(f\"  Has Directional Conflict: {row['Directional_Conflict']}\")\n",
    "        print()\n",
    "\n",
    "def evaluate_navigation_instructions(ground_truths, predictions, directional_weight):\n",
    "    \"\"\"\n",
    "    Main function to evaluate navigation instructions\n",
    "\n",
    "    Args:\n",
    "        ground_truths: List of ground truth instructions\n",
    "        predictions: List of predicted instructions\n",
    "        directional_weight: Weight to give directional terms\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    print(\"Evaluating Navigation Instructions with Directional-Weighted BERTScore\\n\")\n",
    "\n",
    "    # Evaluate with directional weighting\n",
    "    results = enhanced_directional_weighted_bertscore(\n",
    "        ground_truths,\n",
    "        predictions,\n",
    "        directional_weight=directional_weight\n",
    "    )\n",
    "\n",
    "    # Display detailed results\n",
    "    print(\"\\nDetailed Evaluation Results:\")\n",
    "    pd.set_option('display.max_colwidth', None)  # Show full text\n",
    "    print(results[['Pair_ID', 'BERT_F1', 'Enhanced_Directional_Score', 'Weighted_F1', 'Directional_Conflict']].to_string(index=False))\n",
    "\n",
    "    # Visualize results\n",
    "    #visualize_results(results, directional_weight)\n",
    "\n",
    "    # Analyze differences between metrics\n",
    "    #analyze_directional_differences(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff212a-d73f-4c48-9da0-17403d930be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "default_path = os.path.expanduser(\"~/.cache/huggingface/hub/models--Salesforce--blip2-flan-t5-xl/snapshots/0eb0d3b46c14c1f8c7680bca2693baafdb90bb28/\")\n",
    "base_model = Blip2ForConditionalGeneration.from_pretrained(default_path, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "trained_model_path = \"./peft_t_aug/best_model/\"\n",
    "config = PeftConfig.from_pretrained(trained_model_path)\n",
    "trained_model_w_peft = PeftModel.from_pretrained(base_model, trained_model_path)\n",
    "processor = Blip2Processor.from_pretrained(default_path)\n",
    "#device = next(trained_model_w_peft.parameters()).device\n",
    "#print(f\"Model is loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84e48f-db52-40f6-847d-ed10bbba3796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_path = 'datasets_peft/bar/bar_0008.jpg'\n",
    "image = Image.open(img_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1245e3e-8f98-47f0-9ec5-ebbba94dac5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc82d6eb-df38-44e2-a34c-3c5fe0ed2ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Question: How to find the seating area. Answer:\"\n",
    "trained_model_w_peft.to(device)\n",
    "trained_model_w_peft.eval()\n",
    "# base_model.to(device)\n",
    "# base_model.eval()\n",
    "inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
    "#print(inputs)\n",
    "generated_ids = trained_model_w_peft.generate(**inputs,\n",
    "    max_new_tokens=64,\n",
    "    num_beams=5,\n",
    "    min_length=1,\n",
    "    do_sample=False,\n",
    "    temperature=1.0,\n",
    "    repetition_penalty=1,\n",
    "    no_repeat_ngram_size=2\n",
    "                                         \n",
    ")\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1434b9c-a764-43c2-b767-3c4cd01fe423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check input shapes\n",
    "print(f\"Input shape: {inputs.input_ids.shape if hasattr(inputs, 'input_ids') else 'No input_ids'}\")\n",
    "\n",
    "# Print raw generated IDs\n",
    "print(f\"Generated IDs: {generated_ids}\")\n",
    "\n",
    "# Check if generated IDs contain anything meaningful\n",
    "print(f\"Generated IDs length: {len(generated_ids[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391baee1-f8d8-4021-925e-5299191b7ad0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Base model config: {base_model.config}\")\n",
    "print(f\"PEFT config: {config.__dict__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb509c3-fd29-40f9-a971-9912848447f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    all_result = []\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    model.eval()\n",
    "    for data in dataloader:\n",
    "        inputs = data['inputs'].to(device, torch.float16)\n",
    "        generated_ids = model.generate(pixel_values=inputs.pixel_values, \n",
    "                                       input_ids=inputs.input_ids, \n",
    "                                       max_new_tokens=64,\n",
    "                                        num_beams=5,\n",
    "                                        min_length=1,\n",
    "                                        do_sample=False,\n",
    "                                        temperature=1.0,\n",
    "                                        repetition_penalty=1,\n",
    "                                        no_repeat_ngram_size=2\n",
    "                                      )\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "        ground_truth.append(data['answer'][0])\n",
    "        predictions.append(generated_text)\n",
    "        result = [data['question'][0], data['answer'][0], generated_text]\n",
    "        print(\"%30s |label|  %30s  |prediction|  %30s\"%(data['question'][0], data['answer'][0], generated_text))\n",
    "        all_result.append(result)\n",
    "    return all_result, ground_truth, predictions\n",
    "val_dataset = VQADataset(\n",
    "        image_dir=\"datasets_peft\",\n",
    "        annotations_file=\"datasets_peft/test_label.json\",\n",
    "        processor=processor,\n",
    "        max_length=32\n",
    "    )\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e29a7-1209-4deb-94c1-6c65ad548c61",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_result, ground_truth, predictions = evaluate_model(trained_model_w_peft,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cba6d3-5967-48c1-b9af-e7bfba95676d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_results = evaluate_navigation_instructions(ground_truth, predictions, directional_weight=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3ed22-dcf8-4299-956f-80690bc572da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
